{
  
    
        "post0": {
            "title": "Is Upside-Down Reinforcement Learning = Imitation Learning?",
            "content": "Paper • Github Repo . What is Upside-Down Reinforcement Learning? . I found out about this work while attending an RL workshop at NeurIPS 2019. Honestly, it was one of the coolest ideas I stumbled upon at the conference. You can take a look at my other favourite ideas from the conference here. Anyway, this post is about inspecting upside-down reinforcement learning more. Here’s what the abstract says: . Traditional Reinforcement Learning (RL) algorithms either predict rewards with value functions or maximize them using policy search. We study an alternative: Upside-Down Reinforcement Learning (Upside-Down RL or UDRL), that solves RL problems primarily using supervised learning techniques. Here we present the first concrete implementation of UDRL and demonstrate its feasibility on certain episodic learning problems. Experimental results show that its performance can be surprisingly competitive with, and even exceed that of traditional baseline algorithms developed over decades of research. . If you want to deep dive into the paper and understand it more, you can watch this excellent video. But tl;dr - they’ve devised a new supervised learning algorithm to solve reinforcement learning tasks. No policy gradients, no value function estimations, just plain old supervised learning. Here’s a figure from the paper to illustrate this better: . . What the behavior function is trying to predict is — given this observation, what is the best action to take to achieve this desired return (total reward) in this desired horizon (total timesteps). Instead of learning/modeling rewards they’re used as inputs (commands) to directly predict actions. . To understand the overall algorithm better, I ran a few quick experiments. This post documents my findings. . In particular, I wanted to answer the following two questions: . Since implementing most RL algorithms is extremely non-trivial, how easy is to get this algorithm up and running? . | Is it just a cleverly disguised Imitation Learning algorithm? . | Task to solve — Sparse Lunar Lander . To answer the above questions, I implemented the algorithm to solve the Sparse Lunar Lander task (one of the tasks mentioned in the paper). The task is to learn an agent that is able to successfully land a lunar lover as shown below: . . I manually converted the “LunarLander-v2” environment from OpenAI Gym to a sparse one by modifying the reward function as follows: . 0 reward for all non-terminal steps | total episode reward for the final terminal step | . Implementation notes . All the learning components were implemented using Pytorch | The paper lists down all the hyper-parameter values they swept for their experiments. I did not tune them at all; just picked the middle values for each one. | Model architecture: . . | Used comet.ml for metric tracking | . Code for this experiment can be accessed here. . Now, over to the experiment findings. . How easy is it to make the algorithm work? . Answer - surprisingly quick. I was able to get the following reward curve in just my third(!) run attempt; without needing to tinker around a lot with the hyper-parameters. I don’t remember the last time that happened for any of the policy gradient based algorithms. . . Of course, this needs to be taken with a grain of salt because “Lunar Lander” is a relatively simple task. But if you look at the implemented algorithm in code, you’ll agree that the algorithm is ridiculously simple! . Is it just Imitation Learning in disguise? . My first impression on reading the paper was that the idea sounds very similar to Imitation Learning. Consider the following excerpt from the paper describing the replay buffer strategy: . . So as training progresses, by design the trajectories stored in the buffer start looking more and more like expert trajectories (episodes with high returns). And learning just a mapping from states to actions on these expert trajectories (re: Imitation Learning) should suffice. This idea is actually better expressed in this prior work called — Self-Imitation Learning. . To test this hypothesis, I ran the following experiment — trying to learn the optimal actions to take by masking out the command inputs of the behavior function (keeping everything else identical). So effectively, just learning to predict actions from just the observations alone. And here’s what happened: . . Clearly, having commands as inputs to the behavior function make a difference. My guess is that having these commands help the agent further distinguish different types of high return trajectories, in turn helping it learn faster. This obviously needs to be further tested on more complex environments, but the results from my micro-experiment are definitely encouraging. .",
            "url": "https://bprabhakar.github.io/blog/reinforcement%20learning/research/2020/04/18/udrl.html",
            "relUrl": "/reinforcement%20learning/research/2020/04/18/udrl.html",
            "date": " • Apr 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Most surprising ideas from NeurIPS 2019",
            "content": "There are a lot of blogs and articles on the internet that did an excellent job at summarizing the work published at NeurIPS (definitely a lot better than me!). For e.g., I highly recommend checking this out for excellent notes. . As for this post, I’ve picked papers that I personally found to be extremely interesting and to a large degree very counter-intuitive. These are definitely not theoretically ground breaking, but instead challenge the status quo in a very fascinating manner. . So here are my top four picks: . . . . Training Agents using Upside-Down Reinforcement Learning . Summarized Abstract . Traditional Reinforcement Learning (RL) algorithms either predict rewards with value functions or maximize them using policy search. We study an alternative: Upside-Down Reinforcement Learning (Upside-Down RL or UDRL), that solves RL problems primarily using supervised learning techniques. Here we present the first concrete implementation of UDRL and demonstrate its feasibility on certain episodic learning problems. Experimental results show that its performance can be surprisingly competitive with, and even exceed that of traditional baseline algorithms developed over decades of research. Full Paper . Why’s this interesting? . Personally, I found this to be the coolest idea from the conference (and really simple too!). The bottomline is that at this moment, solving RL problems is incredibly difficult, like really difficult. There are all sorts of issues with existing algorithms like, and you can read an excellent article discussing them here - Deep RL Doesn’t Work Yet. The unfortunate truth is that we’re very far off from having a really good general-purpose learning algorithm for Reinforcement Learning problems, as compared to Supervised Learning problems. RL problems are formulated as Markov Decision Processes (MDPs), and this paper proposes to completely change the learning problem formulation itself, i.e., from MDP → Supervised Learning. This could be game-changing because supervised learning is an extremely well studied space and as a result we have incredibly stable learning mechanisms. This clever re-formulation allows us to frame an RL problem as a classification problem. It’s too soon to say if this will work for all kinds of problems, but there’s hope. And if this works, we could finally have a family of algorithms that work off-the-shelf of RL problems. Just as how comfortable we are in modelling “classification” problems, we might finally be able to do the same for “RL” problems. . Another concurrent work coming out of Berkeley lab focussing on the same core idea. In their words - “the method we get from this is not as effective as state-of-the-art RL, but it is simple – just supervised learning conditioned on rewards”. . . . Weight Agnostic Neural Networks . Summarized Abstract . Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We find architectures that can achieve much higher than chance accuracy on MNIST using random weights. Full Paper Slides Blogpost . Why’s this interesting? . Their core thesis is that the neural network architectures alone are powerful enough to solve a problem, irrespective of what the weights of the NN parameters are. And we’ve seen this to be true so far, for e.g., images are processed best by convolutional layer architectures, LSTM-like networks work very well on sequential data. But when it comes to tabular data (the kind that most of the enterprises deal with for various predictions - LTV, Churn etc.), we still resort to standard feed forward architectures. My hypothesis is that we’re yet to discover the right “architectural block” for such kind of structured/tabular data that will just make Deep Learning magically work. This work reinforces my belief because clearly, the architecture alone seems to be making such a crucial difference. There’s been some recent work on this by researchers at Google like TabNet, but it’ll be interesting to see what sort of architectural blocks can be discovered programmatically. . . . Putting an End to End-to-End: Gradient Isolated Learning of Representations . This paper won the Honorable Mention Outstanding New Directions Paper Award. Check out the list of all the other award winning papers here. . Summarized Abstract . We propose a novel deep learning method for local self-supervised representation learning that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. Inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, we split a deep neural network into a stack of gradient-isolated modules. Each module is trained to maximally preserve the information of its inputs. Despite this greedy training, we demonstrate that the proposed method enables optimizing modules asynchronously, allowing large-scale distributed training of very deep neural networks on unlabelled datasets. Full Paper . Why’s this interesting? . This work is interesting in the long run. It is inevitable that going forward, as the size of the datasets collected explodes, models are going to get bigger and deeper and we’re going to see more and more of self-supervised learning (re: pre-trained language models). This algorithm would allow distributed training in the true-est sense, where you can distribute each layer(s) across different systems and train asynchronously – making training mega models (for self-supervised tasks) incredibly scalable and compute efficient. I’m still surprised that this works! . . . Learning by Cheating . This paper was actually published at CoRL 2019 conference, but Vlad Koltun (Chief AI Scientist at Intel) presented it during his talk at one of the NeurIPS workshops. . Summarized Abstract . Vision-based urban driving is hard. The autonomous system needs to both learn to see the world (vision) and learn to act (control). We simplify the problem by decomposing it into two stages. We first train an agent that has access to privileged vision information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants and only learns how to act. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. This two-stage training procedure is counter-intuitive at first, but for the first time, achieves a 100% success rate on all tasks on an autonomous driving benchmarks (CARLA). Full Paper Explainer Video . Why’s this interesting? . The conventional wisdom offered these days is to put everything in a single giant neural network and train everything in a single pass. This paper goes against that and demonstrates how inefficient that is; and shows an ingenious way of solving the self-driving problem better by breaking it into modular learning blocks. I’m very curious to see if this trick can be applied to other large and complex machine learning systems which can be modularised into different components. .",
            "url": "https://bprabhakar.github.io/blog/machine%20learning/research/2020/02/05/neurips.html",
            "relUrl": "/machine%20learning/research/2020/02/05/neurips.html",
            "date": " • Feb 5, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://bprabhakar.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://bprabhakar.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://bprabhakar.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}