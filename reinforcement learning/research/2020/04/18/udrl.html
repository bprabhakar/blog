<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Is Upside-Down Reinforcement Learning = Imitation Learning? | Bharat Prabhakar’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Is Upside-Down Reinforcement Learning = Imitation Learning?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper • Github Repo" />
<meta property="og:description" content="Paper • Github Repo" />
<link rel="canonical" href="https://bprabhakar.github.io/blog/reinforcement%20learning/research/2020/04/18/udrl.html" />
<meta property="og:url" content="https://bprabhakar.github.io/blog/reinforcement%20learning/research/2020/04/18/udrl.html" />
<meta property="og:site_name" content="Bharat Prabhakar’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-18T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://bprabhakar.github.io/blog/reinforcement%20learning/research/2020/04/18/udrl.html","@type":"BlogPosting","headline":"Is Upside-Down Reinforcement Learning = Imitation Learning?","dateModified":"2020-04-18T00:00:00-05:00","datePublished":"2020-04-18T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://bprabhakar.github.io/blog/reinforcement%20learning/research/2020/04/18/udrl.html"},"description":"Paper • Github Repo","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://bprabhakar.github.io/blog/feed.xml" title="Bharat Prabhakar's blog" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-157796734-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>

<link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Bharat Prabhakar&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Is Upside-Down Reinforcement Learning = Imitation Learning?</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-04-18T00:00:00-05:00" itemprop="datePublished">
        Apr 18, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#Reinforcement Learning">Reinforcement Learning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Research">Research</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#what-is-upside-down-reinforcement-learning">What is Upside-Down Reinforcement Learning?</a></li>
<li class="toc-entry toc-h2"><a href="#task-to-solve--sparse-lunar-lander">Task to solve — Sparse Lunar Lander</a></li>
<li class="toc-entry toc-h2"><a href="#implementation-notes">Implementation notes</a></li>
<li class="toc-entry toc-h2"><a href="#how-easy-is-it-to-make-the-algorithm-work">How easy is it to make the algorithm work?</a></li>
<li class="toc-entry toc-h2"><a href="#is-it-just-imitation-learning-in-disguise">Is it just Imitation Learning in disguise?</a></li>
</ul><p align="center">
<a href="https://arxiv.org/abs/1912.02877">Paper</a>
•  
<a href="https://github.com/bprabhakar/upside-down-reinforcement-learning">Github Repo</a>
</p>

<h2 id="what-is-upside-down-reinforcement-learning">
<a class="anchor" href="#what-is-upside-down-reinforcement-learning" aria-hidden="true"><span class="octicon octicon-link"></span></a>What is Upside-Down Reinforcement Learning?</h2>

<p>I found out about this work while attending an RL workshop at NeurIPS 2019. Honestly, it was one of the coolest ideas I stumbled upon at the conference. You can take a look at my other favourite ideas from the conference <a href="https://bprabhakar.github.io/2020/02/05/neurips.html">here</a>. Anyway, this post is about inspecting upside-down reinforcement learning more. Here’s what the abstract says:</p>

<blockquote>
  <p><em>Traditional Reinforcement Learning (RL) algorithms either predict rewards with value functions or maximize them using policy search. We study an alternative: Upside-Down Reinforcement Learning (Upside-Down RL or UDRL), that solves RL problems primarily using supervised learning techniques. Here we present the first concrete implementation of UDRL and demonstrate its feasibility on certain episodic learning problems. Experimental results show that its performance can be surprisingly competitive with, and even exceed that of traditional baseline algorithms developed over decades of research.</em></p>
</blockquote>

<p>If you want to deep dive into the paper and understand it more, you can watch this excellent <a href="https://www.youtube.com/watch?v=RrvC8YW0pT0">video</a>. But tl;dr - they’ve devised a new supervised learning algorithm to solve reinforcement learning tasks. No policy gradients, no value function estimations, just plain old supervised learning. Here’s a figure from the paper to illustrate this better:</p>

<p><img src="https://raw.githubusercontent.com/bprabhakar/blog/master/images/udrl/Untitled.png" alt="Untitled"></p>

<p>What the behavior function is trying to predict is — <em>given this observation, what is the best action to take to achieve this desired return (total reward) in this desired horizon (total timesteps)</em>. Instead of learning/modeling rewards they’re used as inputs (commands) to directly predict actions.</p>

<p>To understand the overall algorithm better, I ran a few quick experiments. This post documents my findings.</p>

<p>In particular, I wanted to answer the following two questions:</p>

<ol>
  <li>
    <p>Since implementing most RL algorithms is extremely non-trivial, how easy is to get this algorithm up and running?</p>
  </li>
  <li>
    <p>Is it just a cleverly disguised Imitation Learning algorithm?</p>
  </li>
</ol>

<h2 id="task-to-solve--sparse-lunar-lander">
<a class="anchor" href="#task-to-solve--sparse-lunar-lander" aria-hidden="true"><span class="octicon octicon-link"></span></a>Task to solve — <em>Sparse Lunar Lander</em>
</h2>

<p>To answer the above questions, I implemented the algorithm to solve the Sparse Lunar Lander task (one of the tasks mentioned in the paper). The task is to learn an agent that is able to successfully land a lunar lover as shown below:</p>

<p><img src="https://raw.githubusercontent.com/bprabhakar/blog/master/images/udrl/lunar-lander-demo.gif" alt="Is%20Upside%20Down%20Reinforcement%20Learning%20Imitation%20Le/lunar-lander-demo.gif"></p>

<p>I manually converted the “LunarLander-v2” environment from OpenAI Gym to a sparse one by modifying the reward function as follows:</p>

<ul>
  <li>0 reward for all non-terminal steps</li>
  <li>total episode reward for the final terminal step</li>
</ul>

<h2 id="implementation-notes">
<a class="anchor" href="#implementation-notes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementation notes</h2>

<ul>
  <li>All the learning components were implemented using Pytorch</li>
  <li>The paper lists down all the hyper-parameter values they swept for their experiments. I did not tune them at all; just picked the middle values for each one.</li>
  <li>
    <p>Model architecture:</p>

    <p><img src="https://raw.githubusercontent.com/bprabhakar/blog/master/images/udrl/NN.png" alt="Is%20Upside%20Down%20Reinforcement%20Learning%20Imitation%20Le/Untitled%201.png"></p>
  </li>
  <li>Used <a href="http://comet.ml/">comet.ml</a> for metric tracking</li>
</ul>

<p>Code for this experiment can be accessed <a href="https://github.com/bprabhakar/upside-down-reinforcement-learning">here</a>.</p>

<p>Now, over to the experiment findings.</p>

<h2 id="how-easy-is-it-to-make-the-algorithm-work">
<a class="anchor" href="#how-easy-is-it-to-make-the-algorithm-work" aria-hidden="true"><span class="octicon octicon-link"></span></a>How easy is it to make the algorithm work?</h2>

<p>Answer - surprisingly quick. I was able to get the following reward curve in just my <strong>third</strong>(!) run attempt; without needing to tinker around a lot with the hyper-parameters. I don’t remember the last time that happened for any of the policy gradient based algorithms.</p>

<p><img src="https://raw.githubusercontent.com/bprabhakar/blog/master/images/udrl/sparse_lunar_lander.png" alt="Is%20Upside%20Down%20Reinforcement%20Learning%20Imitation%20Le/sparse_lunar_lander.png"></p>

<p>Of course, this needs to be taken with a grain of salt because “Lunar Lander” is a relatively simple task. But if you look at the implemented algorithm in code, you’ll agree that the algorithm is ridiculously simple!</p>

<h2 id="is-it-just-imitation-learning-in-disguise">
<a class="anchor" href="#is-it-just-imitation-learning-in-disguise" aria-hidden="true"><span class="octicon octicon-link"></span></a>Is it just Imitation Learning in disguise?</h2>

<p>My first impression on reading the paper was that the idea sounds very similar to Imitation Learning. Consider the following excerpt from the paper describing the replay buffer strategy:</p>

<p><img src="https://raw.githubusercontent.com/bprabhakar/blog/master/images/udrl/replay_buffer.png" alt="Is%20Upside%20Down%20Reinforcement%20Learning%20Imitation%20Le/Untitled%202.png"></p>

<p>So as training progresses, by design the trajectories stored in the buffer start looking more and more like <em>expert trajectories</em> (episodes with high returns). And learning just a mapping from states to actions on these expert trajectories (re: Imitation Learning) should suffice. This idea is actually better expressed in this prior work called — <a href="https://arxiv.org/abs/1806.05635">Self-Imitation Learning</a>.</p>

<p>To test this hypothesis, I ran the following experiment — trying to learn the optimal actions to take by <strong>masking out the command inputs</strong> of the behavior function (keeping everything else identical). So effectively, just learning to predict actions from just the observations alone. And here’s what happened:</p>

<p><img src="https://raw.githubusercontent.com/bprabhakar/blog/master/images/udrl/sparse_lunar_lander_masked_cmd.jpeg" alt="Is%20Upside%20Down%20Reinforcement%20Learning%20Imitation%20Le/sparse_lunar_lander_masked_cmd.jpeg"></p>

<p>Clearly, having <em>commands</em> as inputs to the behavior function make a difference. My guess is that having these commands help the agent further distinguish different types of high return trajectories, in turn helping it learn faster. This obviously needs to be further tested on more complex environments, but the results from my micro-experiment are definitely encouraging.</p>

  </div><a class="u-url" href="/blog/reinforcement%20learning/research/2020/04/18/udrl.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Musings on the latest in machine learning.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/bprabhakar" title="bprabhakar"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/aglooka" title="aglooka"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
